{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tqdm\n",
    "import scipy\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General plan\n",
    "\n",
    "- Read raw velocity data, make some features out of it\n",
    "\n",
    "- Try simple classifier with where features can be interpreted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read them all\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "- there is `data` folder in the root of the project (its not under SC).\n",
    "\n",
    "- there are 2 types of file `activity-xyz.json` and `stream-xyz.json`,  with the first one having activity general and aggregated metadata and the second one is more detailed lat lon velocity and timestamp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id(s, prefix):\n",
    "    m = re.search(\"%s-(.+)\\.json\" % prefix, s)\n",
    "    try:\n",
    "        return m.group(1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def process_strava_activity(**kwargs) -> List[Dict]:\n",
    "    res = dict([(c, kwargs['stream_json'].get(c, np.nan)) for c in kwargs['cols']])\n",
    "    res['id'] = kwargs['_id']\n",
    "    return [res]    \n",
    "\n",
    "def strava_file_reader(stream_file, data_path, stream_processor) -> List[Dict]:\n",
    "    with open(os.path.join(data_path, stream_file), 'r') as f:\n",
    "        prefix = stream_file.split(\"-\")[0]\n",
    "        activity_id = get_id(stream_file, prefix)        \n",
    "        return stream_processor(stream_json=json.load(f), _id=activity_id)\n",
    "    \n",
    "assert get_id(\"activity-2342134134.json\", \"activity\") == \"2342134134\"\n",
    "assert get_id(\"stream-999935253.json\", \"stream\") == \"999935253\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(sys.path[0], \"../data\")\n",
    "activity_file_list = [file for file in os.listdir(data_path) if fnmatch.fnmatch(file, 'activity-*.json')]\n",
    "stream_file_list = [file for file in os.listdir(data_path) if fnmatch.fnmatch(file, 'stream-*.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Activities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "original_cols = ['max_speed','commute','distance','has_heartrate','average_speed','device_watts', 'id']\n",
    "\n",
    "more_cols = ['premium', 'average_cadence', 'average_temp', 'latlng',\n",
    "             'average_heartrate', 'average_watts', 'kilojoules', 'max_speed', 'manual', 'moving_time', 'elapsed_time']\n",
    "\n",
    "activity_proc = lambda stream_json, _id: process_strava_activity(stream_json=stream_json, _id=_id, cols=original_cols + more_cols)\n",
    "\n",
    "strava_activity_gen = (s for f in tqdm.tqdm_notebook(activity_file_list[:]) \n",
    "                         for s in strava_file_reader(f, data_path, activity_proc))\n",
    "\n",
    "activity_df = pd.DataFrame(strava_activity_gen).set_index('id')\n",
    "activity_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Streams\n",
    "\n",
    "We can't read all streams in raw format (well we can, but we assume this notebook can be \"tried at home\")\n",
    "\n",
    "Therefore we read the stream, convert it into features, and join these stream features with the activity df above somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_strava_stream(**kwargs) -> List[Dict]:\n",
    "    stream = kwargs['stream_json']    \n",
    "    \n",
    "    latlng = [ll for ll in stream.get('latlng', [])]\n",
    "    lats = [l[0] for l in latlng]\n",
    "    lons = [l[1] for l in latlng]        \n",
    "    \n",
    "    relative_time = [t for t in stream.get('time',[])]\n",
    "    distance = [d for d in stream.get('distance',[])]\n",
    "    velocity_smooth = [v for v in stream.get('velocity_smooth', [])]       \n",
    "    \n",
    "    time_index = pd.TimedeltaIndex(np.roll(relative_time, shift=1), unit='s')\n",
    "    \n",
    "    data = pd.DataFrame([(z[0], z[1], z[2], z[3]) for z in \n",
    "                          zip(distance, velocity_smooth, lats, lons)], \n",
    "                        columns=['distance', 'velocity', 'lat', 'lon'], index=time_index)    \n",
    "    data = data.iloc[1:]    \n",
    "    \n",
    "    return [{'data': data, 'id':kwargs['_id']}]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Speed time series feature ideas  (naive not very deep in domain):\n",
    "\n",
    "\n",
    "\n",
    "### The features form Velocity Time Series\n",
    "\n",
    "`we resample the incoming velocities with 10s intervals`\n",
    "\n",
    "1. number of na-s (disclaimer: it is helpful)\n",
    "\n",
    "2. slow speed sections (disclaimer: it is helpful)\n",
    "\n",
    "3. Diff between instant value and some moving average, mean and std of it: (disclaimer: it is helpful)\n",
    "\n",
    "4. Time lag corresponding to the first zero of the auto-correlation function, based on moving average\n",
    "\n",
    "   (disclaimer: it is NOT helpful, but sounds smart, we take it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_zero(_y, sampling_rate=1):\n",
    "    for i, z in enumerate(_y[1:]):\n",
    "        if _y[i-1] >=0 and z <= 0:\n",
    "            return i*sampling_rate\n",
    "        else:\n",
    "            continue            \n",
    "    return None\n",
    "\n",
    "\n",
    "def featurise(raw_stream: pd.DataFrame, stream_id:str) -> Dict:    \n",
    "    try:    \n",
    "        resampled_stream = raw_stream.resample('10s').mean()\n",
    "\n",
    "        feature_nan_fraction = np.sum(resampled_stream.velocity.isnull()) / len(resampled_stream)\n",
    "\n",
    "        slow_speed_fraction = np.sum(resampled_stream.velocity < 1) / len(resampled_stream)\n",
    "\n",
    "        mav1m = resampled_stream.rolling('1min', min_periods=4).mean()\n",
    "        mav5m = resampled_stream.rolling('5min', min_periods=20).mean()\n",
    "        mav10m = resampled_stream.rolling('10min', min_periods=40).mean()\n",
    "\n",
    "        velcoty_diff_1m = (raw_stream.velocity - mav1m.velocity).dropna()\n",
    "        velcoty_diff_5m = (raw_stream.velocity - mav5m.velocity).dropna()\n",
    "        velcoty_diff_10m = (raw_stream.velocity - mav10m.velocity).dropna()\n",
    "\n",
    "        # ac stuff    \n",
    "        def acf_zero(s, x = range(0, 2000)):\n",
    "            y = [s.autocorr(lag=l) for l in x]\n",
    "            return find_zero(y)\n",
    "\n",
    "        return {\n",
    "            'sream_id': stream_id,\n",
    "            'nan_fraction': feature_nan_fraction,\n",
    "            'slow_speed_fraction': slow_speed_fraction,\n",
    "\n",
    "            'velocitydiff_std_1m': velcoty_diff_1m.std(),\n",
    "            'velocitydiff_std_5m': velcoty_diff_5m.std(),\n",
    "            'velocitydiff_std_10m': velcoty_diff_10m.std(),\n",
    "\n",
    "            'velocitydiff_mean_1m': velcoty_diff_1m.mean(),\n",
    "            'velocitydiff_mean_5m': velcoty_diff_5m.mean(),\n",
    "            'velocitydiff_mean_10m': velcoty_diff_10m.mean(),\n",
    "\n",
    "            'acf0_1m': acf_zero(mav1m.velocity),\n",
    "            'acf0_5m': acf_zero(mav5m.velocity),\n",
    "            'acf0_10m': acf_zero(mav10m.velocity),        \n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Warn: %s\" % e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# be careful est. calculation time on 8k samples is 6 hours, don't try at home, use pickle\n",
    "stream_features = pd.DataFrame(( featurise(s['data'], s['id'])\n",
    "    for f in tqdm.tqdm_notebook(stream_file_list[:])\n",
    "    for s in strava_file_reader(f, data_path, process_strava_stream))).set_index('sream_id')\n",
    "\n",
    "stream_features.to_pickle(\"stream_features\", compression=\"bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activity_df.to_pickle(\"activity_df\", compression=\"bz2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
