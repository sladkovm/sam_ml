{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/sladkovm/Dropbox/Projects/sam_ml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sam_ml import RemoveOutliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9609 entries, 0 to 9609\n",
      "Data columns (total 9 columns):\n",
      "average_speed    9609 non-null float64\n",
      "commute          9609 non-null bool\n",
      "device_watts     9609 non-null object\n",
      "distance         9609 non-null float64\n",
      "elapsed_time     9609 non-null int64\n",
      "has_heartrate    9609 non-null bool\n",
      "id               9609 non-null int64\n",
      "max_speed        9609 non-null float64\n",
      "moving_time      9609 non-null int64\n",
      "dtypes: bool(2), float64(3), int64(3), object(1)\n",
      "memory usage: 619.3+ KB\n",
      "CPU times: user 991 ms, sys: 482 ms, total: 1.47 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if os.sys.platform == 'win32':\n",
    "    data_path = os.path.normpath(r'C:\\Users\\msladkov\\Dropbox\\Projects\\activity-maps\\brussels-data')\n",
    "else:\n",
    "    data_path = os.path.normpath(r'/Users/sladkovm/Dropbox/Projects/sam_ml/data/brussels-data')\n",
    "    \n",
    "file_list = [file for file in os.listdir(data_path) if fnmatch.fnmatch(file, 'activity-*.json')]\n",
    "\n",
    "keys = ['max_speed','commute','distance','has_heartrate','average_speed','device_watts',\n",
    "        'elapsed_time', 'moving_time',\n",
    "        'id']\n",
    "\n",
    "data_list = []\n",
    "for file in file_list:\n",
    "    f_name = os.path.join(data_path, file)\n",
    "    with open(f_name, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "        res = {}\n",
    "        for k in keys:\n",
    "            try:\n",
    "                res[k]=content[k]\n",
    "            except:\n",
    "                res[k] = np.nan\n",
    "        data_list.append(res)\n",
    "        \n",
    "df = pd.DataFrame(data_list)\n",
    "df = df.dropna()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9368 entries, 0 to 9609\n",
      "Data columns (total 9 columns):\n",
      "average_speed    9368 non-null float64\n",
      "commute          9368 non-null bool\n",
      "device_watts     9368 non-null object\n",
      "distance         9368 non-null float64\n",
      "elapsed_time     9368 non-null int64\n",
      "has_heartrate    9368 non-null bool\n",
      "id               9368 non-null int64\n",
      "max_speed        9368 non-null float64\n",
      "moving_time      9368 non-null int64\n",
      "dtypes: bool(2), float64(3), int64(3), object(1)\n",
      "memory usage: 603.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ro = RemoveOutliers()\n",
    "ro = ro.fit(df[['distance', 'average_speed', 'max_speed', 'elapsed_time', 'moving_time']])\n",
    "df_transform = ro.transform(df)\n",
    "df_transform.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add stop_to_move_ratio column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all absent moving time and replace with a value of elapsed time\n",
    "# df.loc[df['moving_time']<=0, 'moving_time'] = df.loc[df['moving_time']<=0, 'elapsed_time']\n",
    "\n",
    "# Find all elapsed_time smaller than moving_time and set the equal\n",
    "# df.loc[df['elapsed_time'] < df['moving_time'],\n",
    "#        'elapsed_time'] = df.loc[df['elapsed_time'] < df['moving_time'], 'moving_time']\n",
    "\n",
    "# Calculate ratio\n",
    "# df['stop_to_move_ratio'] = (df['elapsed_time'] - df['moving_time'])/df['moving_time']\n",
    "\n",
    "# Set ration >1.0 to 1.0 (the stop time == moving time)\n",
    "# df.loc[df['stop_to_move_ratio']>1.0, 'stop_to_move_ratio'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = df[df['stop_to_move_ratio']<1.0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into Model and Not Categorized set\n",
    "\n",
    "All activities labeled as *commute* are treated as a ground truth set **Commute**\n",
    "\n",
    "In order to get a ground truth **Training** set we will make an assumption that all activities with *device_watts* and not labeled as *commute* are the actual training activities.\n",
    "\n",
    "The rationale behind this assumption is simple - the powermeter is expensive piece of equipment and those people, who have it, most likely use it for training. For those situations where one would go to work via the training route and will come back via the commute route (as I often do) we rely on the deligence of the riders to label the return route as a commute (as I do).\n",
    "\n",
    "With these two assumptions at hand we can extract a model set from the given data, fit the classifier on this data set and use the classifier to label the rest of the data.\n",
    "\n",
    "Stepwise actions:\n",
    "* find all activivities labeled as commute and labels them as 'c'\n",
    "* find all activities with powermeter, but labeled as not a comute and label them as 't' - training\n",
    "* the rest of the activities should be labeled as 'nc' - non categorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx_commute = df['commute']\n",
    "idx_not_commute = df['device_watts'] & ~df['commute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['label'] = 'nc'\n",
    "df.loc[idx_commute, 'label'] = 'c'\n",
    "df.loc[idx_not_commute, 'label'] = 't'\n",
    "df_model = df[df['label']!='nc'].reset_index(drop=True)\n",
    "df_nc = df[df['label']=='nc'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprint('{} commute'.format(len(df[df['label']=='c'])))\n",
    "pprint('{} training'.format(len(df[df['label']=='t'])))\n",
    "pprint('{} not categorized'.format(len(df[df['label']=='nc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and Split model data into Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "training_features = ['distance', 'average_speed', 'max_speed', 'has_heartrate']\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df_model[training_features])\n",
    "# scaler = preprocessing.RobustScaler(with_centering=True,\n",
    "#                                     with_scaling=True,\n",
    "#                                     quantile_range=(25.0, 75.0)).fit(df_model[training_features])\n",
    "\n",
    "scaled_data = scaler.transform(df_model[training_features])\n",
    "\n",
    "train, test, y_train, y_test = model_selection.train_test_split(scaled_data,\n",
    "                                                                df_model['label'].values,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train, columns=training_features)\n",
    "df_train['label'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame(test, columns=training_features)\n",
    "df_test['label'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine training data using the pairwise plot\n",
    "\n",
    "* The major conclusion is that *has_heartrate* alone is not a sufficient discriminant between **commute** and **training** classes. Luckily it still shows some bias (prensence of heartrate) in the **training** class\n",
    "* There is a strong correlation between all tree continuosu variables hinting that effectively we have less than 4 features\n",
    "* Each of the continuos variable does not show a significant difference in **mean** and/or **std** to make it a dominant classifier feature\n",
    "* In conclusion - it would be naive to expect a stellar classifier performance with this set of features, but better than dummyClassifier should be possible.\n",
    "* All tree continuous varibles (bear in mind the strong cross-correlation) do hint on effective separability between classes\n",
    "* The *has_heartrate*, on the other hand, appears to be a very weak discriminant feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1, figsize=(12,6))\n",
    "# pd.plotting.parallel_coordinates(df_train, 'label', ax=ax)\n",
    "sns.set()\n",
    "sns.set_context(font_scale=3)\n",
    "ax = sns.pairplot(df_train, hue='label')\n",
    "# plt.tight_layout()\n",
    "plt.savefig('pairwise.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PCA analysis and visualize the ordered features\n",
    "\n",
    "* No vanishing dimensions in the transformed feature set hinting that we have not enough independent observations \n",
    "* Classification results are likely to be dominated by the strong first component. This is also confirmed by the pairwise plot, where only first component shows strong separation in **mean** between classes\n",
    "* The rest of the components do not appear to be separable meaning we are most likely have a 1-feature classifier. This is confirmed by the parallel coordinates plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(whiten=True).fit(train)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(train), columns=['1', '2', '3', '4'])\n",
    "df_pca['label'] = y_train\n",
    "\n",
    "f, ax = plt.subplots(2,1, figsize=(12,8))\n",
    "\n",
    "sns.barplot(np.arange(len(pca.explained_variance_ratio_)),\n",
    "            pca.explained_variance_ratio_, ax=ax[0])\n",
    "\n",
    "\n",
    "pd.plotting.parallel_coordinates(df_pca, 'label', ax=ax[1])\n",
    "\n",
    "\n",
    "\n",
    "ax = sns.pairplot(df_pca, hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get transformation coefficients PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_contributions = pca.inverse_transform(np.eye(train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "\n",
    "ylabels = ['{:.1f} %'.format(x*100) for x in pca.explained_variance_ratio_]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(np.abs(feature_contributions), annot=True,\n",
    "           xticklabels=training_features,\n",
    "           yticklabels=ylabels,\n",
    "           ax=ax)\n",
    "# plt.ylabel('Explained variance', fontsize=16.0)\n",
    "# plt.xlabel('Original features', fontsize=16.0)\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig('pca.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation analysis of the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1.0)\n",
    "scores = model_selection.cross_val_score(clf, scaled_data,\n",
    "                                         df_model['label'].values, \n",
    "                                         cv=5)\n",
    "print('Accuracy: {:.2f} +/- {:.2f}'.format(scores.mean(), 2*scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear').fit(train, y_train)\n",
    "clf.score(test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify fit on validation set and visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_set = pd.DataFrame(test, columns=training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_set['label'] = y_test\n",
    "validation_set['predictions'] = clf.predict(test)\n",
    "validation_set['error'] = validation_set['label'] != validation_set['predictions']\n",
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,\n",
    "                      validation_set['predictions'].values, ['c', 't'])\n",
    "sns.heatmap(cm, annot=True, fmt='.2f',\n",
    "            xticklabels=['c', 't'],\n",
    "            yticklabels=['c', 't'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_boundary(clf, range_x=(-3.0, 3.0)):\n",
    "    w = clf.coef_[0]\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(range_x[0], range_x[1])\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    return (xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(xx, yy) = decision_boundary(clf,\n",
    "                             range_x = (validation_set['distance'].min(),\n",
    "                                        validation_set['distance'].max()))\n",
    "colors={'c':'b', 't':'g'}\n",
    "\n",
    "f, ax = plt.subplots(1, figsize=(10,10))\n",
    "\n",
    "ax.fill_between(xx, validation_set['average_speed'].min(), yy,\n",
    "               facecolor='blue',\n",
    "               alpha=0.1,\n",
    "               label='commute region')\n",
    "\n",
    "ax.fill_between(xx, yy, validation_set['average_speed'].max(),\n",
    "               facecolor='green',\n",
    "                alpha=0.1,\n",
    "                label='workout region')\n",
    "\n",
    "\n",
    "X='distance'\n",
    "Y='average_speed'\n",
    "\n",
    "## plot hearts\n",
    "ax.scatter(x=validation_set.loc[df_test['has_heartrate']>0, X],\n",
    "            y=validation_set.loc[df_test['has_heartrate']>0,  Y],\n",
    "            marker=r'$\\heartsuit$',\n",
    "            s=200,\n",
    "            facecolor='red',\n",
    "            edgecolor='red',\n",
    "            alpha=0.3,\n",
    "          label='has heartrate')\n",
    "\n",
    "ax.scatter(x=validation_set[X],\n",
    "          y=validation_set[Y],\n",
    "          c =validation_set['predictions'].map(colors),\n",
    "          label='predicted type')\n",
    "\n",
    "\n",
    "\n",
    "ax.scatter(x=validation_set.loc[validation_set['error'], X],\n",
    "           y=validation_set.loc[validation_set['error'],  Y],\n",
    "           marker=r'$\\bigoplus$',\n",
    "           c='red',\n",
    "          label='missclasified type')\n",
    "\n",
    "plt.xlabel('normalized distance', fontsize=16.0)\n",
    "plt.ylabel('normalized speed', fontsize=16.0)\n",
    "plt.legend(fontsize=16.0)\n",
    "\n",
    "plt.ylim((validation_set['average_speed'].min(), validation_set['average_speed'].max()))\n",
    "plt.xlim((validation_set['distance'].min(), validation_set['distance'].max()))\n",
    "\n",
    "f.tight_layout()\n",
    "f.savefig('workout_commute_classifier.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prediction on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale features\n",
    "f_scaled = scaler.fit_transform(df[training_features])\n",
    "# run the classifier\n",
    "df['predictions'] = clf.predict(f_scaled)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(f_scaled, columns=training_features)\n",
    "df_pred['predictions'] = df['predictions']\n",
    "ax = sns.pairplot(df_pred, hue='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['id', 'predictions']].to_csv('brussels_v1.csv', index=False, float_format='%s')\n",
    "df[['id', 'predictions']].to_json('brussels_v1.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# estimator = KMeans(init='k-means++', n_clusters=2)\n",
    "# estimator.fit(df_reduced)\n",
    "\n",
    "# estimator.labels_\n",
    "# df_reduced['labels'] = estimator.labels_\n",
    "# df_reduced['commute'] = df['commute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# color = {0: 'green', 1: 'blue'}\n",
    "# mark = {0: 'c', 1: 't'}\n",
    "\n",
    "# miss = df_reduced['labels'].astype(bool) & df_reduced['commute']\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.scatter(x=df_reduced[0],\n",
    "#            y=df_reduced[1],\n",
    "#            c=[color[x] for x in df_reduced['labels']])\n",
    "# ax.hold=True\n",
    "# ax.scatter(x=df_reduced[0].loc[miss],\n",
    "#            y=df_reduced[1].loc[miss],\n",
    "#            c='red',\n",
    "#            marker='x')\n",
    "\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.scatter(xs=df_reduced[0],\n",
    "#            ys=df_reduced[1],\n",
    "#            zs=df_reduced[2],\n",
    "#            c=[color[x] for x in df_reduced['labels']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
